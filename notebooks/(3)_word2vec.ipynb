{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5430e21",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abde4ee9",
   "metadata": {},
   "source": [
    "IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "713b53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB , GaussianNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score , precision_score , recall_score , f1_score , roc_auc_score , confusion_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32ef46de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>tv future hand viewer home theatre system plas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>worldcom bos leave book alone former worldcom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>tiger wary farrell gamble leicester say rush m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>yeading face newcastle fa cup premiership side...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ocean twelve raid box office ocean twelve crim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text\n",
       "0         4  tv future hand viewer home theatre system plas...\n",
       "1         0  worldcom bos leave book alone former worldcom ...\n",
       "2         3  tiger wary farrell gamble leicester say rush m...\n",
       "3         3  yeading face newcastle fa cup premiership side...\n",
       "4         1  ocean twelve raid box office ocean twelve crim..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/sarthaksharna/Text_Classification/data/cleaned_bbc_text')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4088e",
   "metadata": {},
   "source": [
    "BUILDING THE CORPUS TO TRAIN CUSTOM WORD2VEC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be175127",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [text.split() for text in df['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0632a4",
   "metadata": {},
   "source": [
    "TRAINING WORD2VEC MODEL ON CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "300cd22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = (Word2Vec(corpus , window = 5 , vector_size = 100 , min_count = 2 , epochs = 80 , workers = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a98d4d",
   "metadata": {},
   "source": [
    "EXAMPLE OF PERFORMANCE OF WORD2VEC EMBEDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cf0b000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('economic', 0.6848961710929871),\n",
       " ('export', 0.5342537760734558),\n",
       " ('growth', 0.501365602016449),\n",
       " ('recovery', 0.46114271879196167),\n",
       " ('manufacturing', 0.44830459356307983),\n",
       " ('eurozone', 0.44820621609687805),\n",
       " ('reconstruction', 0.419453501701355),\n",
       " ('tourism', 0.4194468557834625),\n",
       " ('spending', 0.4189000725746155),\n",
       " ('prosperity', 0.4171815514564514)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('economy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ab9db10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16316"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324232d",
   "metadata": {},
   "source": [
    "AVERAGE Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8428e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word2vec(text):\n",
    "    text = [word for word in text if word in model.wv.index_to_key]\n",
    "    if len(text) >= 1:\n",
    "        return np.mean(model.wv[text], axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfa19bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06186217,  0.47195077,  0.46774596,  0.22015978, -0.01070364,\n",
       "       -0.73100764,  0.6814023 ,  1.1034652 , -0.81723744, -0.65028024,\n",
       "       -0.50911295, -0.90445036, -0.02309715, -0.24053508,  0.8122594 ,\n",
       "       -0.27319038, -0.7675168 , -1.2137283 ,  0.26681498, -0.95029753,\n",
       "        0.16705774,  0.54618144,  0.16522108, -1.0695883 , -0.4250347 ,\n",
       "        0.98700804, -1.2009306 ,  0.44621125, -0.6188722 , -0.56393147,\n",
       "        1.3343891 , -0.15970287,  0.0276325 ,  0.60533893,  0.4096876 ,\n",
       "        0.30672482,  0.9676891 , -1.1721765 , -0.4365567 , -0.9912437 ,\n",
       "        0.19740902, -0.48940924, -0.34490502,  0.11961739, -0.06073605,\n",
       "       -1.1481203 , -0.53604704,  1.014435  ,  0.10032601, -0.14339465,\n",
       "        1.0143349 , -0.45714152, -0.04736331,  0.28960285, -0.5851607 ,\n",
       "        0.20471282,  1.0107883 ,  0.0042961 , -0.05704316,  0.66864914,\n",
       "        0.16763914,  1.8738737 , -0.6079127 ,  0.13694963, -1.4475131 ,\n",
       "        0.20606405,  0.979309  , -0.05338528, -1.0411755 ,  1.68912   ,\n",
       "       -0.55821645,  0.3374564 ,  1.4295213 , -0.42758763, -0.00789864,\n",
       "        0.38309127,  0.11913455, -0.5027067 , -1.1364262 , -0.30559716,\n",
       "       -0.8068087 , -0.01771858, -0.6351593 ,  1.2128171 ,  0.2853394 ,\n",
       "        0.09050112, -0.62502015,  2.5323334 ,  1.5366958 ,  0.34812346,\n",
       "        1.0511214 ,  1.1005235 ,  0.27190426,  0.32527328,  1.9293835 ,\n",
       "        0.84869576,  0.5596408 , -1.0706761 ,  0.21151549,  0.68348044],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_word2vec(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df61aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd87cbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2118/2118 [03:13<00:00, 10.93it/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "\n",
    "for text in tqdm(df['text'].values):\n",
    "    X.append(avg_word2vec(text))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dde3e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d27869a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2118, 100)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6fbaac9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06186217,  0.47195077,  0.46774596,  0.22015978, -0.01070364,\n",
       "       -0.73100764,  0.6814023 ,  1.1034652 , -0.81723744, -0.65028024,\n",
       "       -0.50911295, -0.90445036, -0.02309715, -0.24053508,  0.8122594 ,\n",
       "       -0.27319038, -0.7675168 , -1.2137283 ,  0.26681498, -0.95029753,\n",
       "        0.16705774,  0.54618144,  0.16522108, -1.0695883 , -0.4250347 ,\n",
       "        0.98700804, -1.2009306 ,  0.44621125, -0.6188722 , -0.56393147,\n",
       "        1.3343891 , -0.15970287,  0.0276325 ,  0.60533893,  0.4096876 ,\n",
       "        0.30672482,  0.9676891 , -1.1721765 , -0.4365567 , -0.9912437 ,\n",
       "        0.19740902, -0.48940924, -0.34490502,  0.11961739, -0.06073605,\n",
       "       -1.1481203 , -0.53604704,  1.014435  ,  0.10032601, -0.14339465,\n",
       "        1.0143349 , -0.45714152, -0.04736331,  0.28960285, -0.5851607 ,\n",
       "        0.20471282,  1.0107883 ,  0.0042961 , -0.05704316,  0.66864914,\n",
       "        0.16763914,  1.8738737 , -0.6079127 ,  0.13694963, -1.4475131 ,\n",
       "        0.20606405,  0.979309  , -0.05338528, -1.0411755 ,  1.68912   ,\n",
       "       -0.55821645,  0.3374564 ,  1.4295213 , -0.42758763, -0.00789864,\n",
       "        0.38309127,  0.11913455, -0.5027067 , -1.1364262 , -0.30559716,\n",
       "       -0.8068087 , -0.01771858, -0.6351593 ,  1.2128171 ,  0.2853394 ,\n",
       "        0.09050112, -0.62502015,  2.5323334 ,  1.5366958 ,  0.34812346,\n",
       "        1.0511214 ,  1.1005235 ,  0.27190426,  0.32527328,  1.9293835 ,\n",
       "        0.84869576,  0.5596408 , -1.0706761 ,  0.21151549,  0.68348044],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa46868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEPENDENT VARIABLE\n",
    "y = df['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d341d4",
   "metadata": {},
   "source": [
    "TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50812ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1694, 100), (424, 100), (1694,), (424,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.2 , random_state = 42)\n",
    "\n",
    "X_train.shape , X_test.shape , y_train.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5bf6ca",
   "metadata": {},
   "source": [
    "TRAINING MODELS IN PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "632166af",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \n",
    "    'Logistic Regression' : LogisticRegression(),\n",
    "    'Random Forest' : RandomForestClassifier(),\n",
    "    'GradientBoostingClassifier' : GradientBoostingClassifier(),\n",
    "    'XGBClassifier' : XGBClassifier()\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9c1168b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : Logistic Regression \n",
      " \n",
      "Train Accuracy : 0.5867768595041323\n",
      "Train Precision : 0.5749562953418123\n",
      "Train Recall : 0.5672787995254541\n",
      "Train F1-Score : 0.5671068656176864\n",
      "\n",
      "\n",
      "Test Accuracy : 0.5825471698113207\n",
      "Test Precision : 0.5820957911867003\n",
      "Test Recall : 0.5693087798739623\n",
      "Test F1-Score : 0.5624900406109541 \n",
      "\n",
      "confusion_matrix : [[73  5  8  6  5]\n",
      " [16 25  6 24 10]\n",
      " [12  4 43 14  5]\n",
      " [ 8  5  3 69  9]\n",
      " [12  3 14  8 37]] \n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "Model : Random Forest \n",
      " \n",
      "Train Accuracy : 1.0\n",
      "Train Precision : 1.0\n",
      "Train Recall : 1.0\n",
      "Train F1-Score : 1.0\n",
      "\n",
      "\n",
      "Test Accuracy : 0.5683962264150944\n",
      "Test Precision : 0.5661136656715604\n",
      "Test Recall : 0.5549799916620125\n",
      "Test F1-Score : 0.5479176566123474 \n",
      "\n",
      "confusion_matrix : [[71  9 10  4  3]\n",
      " [18 23 15 20  5]\n",
      " [11  5 42 15  5]\n",
      " [11  2  4 69  8]\n",
      " [14  5 10  9 36]] \n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "Model : GradientBoostingClassifier \n",
      " \n",
      "Train Accuracy : 0.9569067296340024\n",
      "Train Precision : 0.9586711063302598\n",
      "Train Recall : 0.9546354598302618\n",
      "Train F1-Score : 0.9564484531404307\n",
      "\n",
      "\n",
      "Test Accuracy : 0.589622641509434\n",
      "Test Precision : 0.587031857031857\n",
      "Test Recall : 0.5785113210352664\n",
      "Test F1-Score : 0.578257222041018 \n",
      "\n",
      "confusion_matrix : [[68 11  7  5  6]\n",
      " [15 37  7 16  6]\n",
      " [14  6 38 13  7]\n",
      " [ 6  7  4 69  8]\n",
      " [17  4 10  5 38]] \n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "Model : XGBClassifier \n",
      " \n",
      "Train Accuracy : 1.0\n",
      "Train Precision : 1.0\n",
      "Train Recall : 1.0\n",
      "Train F1-Score : 1.0\n",
      "\n",
      "\n",
      "Test Accuracy : 0.6061320754716981\n",
      "Test Precision : 0.6013026369840141\n",
      "Test Recall : 0.5966086873595831\n",
      "Test F1-Score : 0.5955052505500161 \n",
      "\n",
      "confusion_matrix : [[68 16  4  6  3]\n",
      " [15 35 10 12  9]\n",
      " [10  8 39 12  9]\n",
      " [10  3  2 71  8]\n",
      " [12  7  7  4 44]] \n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name , clf in models.items() :\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            ('classifier' , clf)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipe.fit(X_train , y_train)\n",
    "\n",
    "    y_pred_train = pipe.predict(X_train)\n",
    "    y_pred_test = pipe.predict(X_test)\n",
    "\n",
    "    print(f'Model : {model_name} \\n ')\n",
    "\n",
    "    print(f'Train Accuracy : {accuracy_score(y_train , y_pred_train)}')\n",
    "    print(f'Train Precision : {precision_score(y_train , y_pred_train , average = \"macro\")}')\n",
    "    print(f'Train Recall : {recall_score(y_train , y_pred_train , average = \"macro\")}')\n",
    "    print(f'Train F1-Score : {f1_score(y_train , y_pred_train , average = \"macro\")}')\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    print(f'Test Accuracy : {accuracy_score(y_test , y_pred_test)}')\n",
    "    print(f'Test Precision : {precision_score(y_test , y_pred_test , average = \"macro\")}')\n",
    "    print(f'Test Recall : {recall_score(y_test , y_pred_test , average = \"macro\")}')\n",
    "    print(f'Test F1-Score : {f1_score(y_test , y_pred_test , average = \"macro\")}' , '\\n')\n",
    "\n",
    "    print(f'confusion_matrix : {confusion_matrix(y_test , y_pred_test)}' , '\\n')\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    print('=='*50)\n",
    "\n",
    "    print('\\n')      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
